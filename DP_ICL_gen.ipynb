{"cells":[{"cell_type":"markdown","source":["# Preparing\n"],"metadata":{"id":"abD4VkvbQaS_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbiU1_7hmDya","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710031848341,"user_tz":300,"elapsed":36757,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"0643945f-22ef-4a41-a544-7087cbc98965"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","Collecting evaluate\n","  Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n","Collecting datasets>=2.0.0 (from evaluate)\n","  Using cached datasets-2.18.0-py3-none-any.whl (510 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n","Collecting dill (from evaluate)\n","  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n","Collecting multiprocess (from evaluate)\n","  Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n","Collecting responses<0.19 (from evaluate)\n","  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n","Installing collected packages: dill, responses, multiprocess, datasets, evaluate\n","Successfully installed datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=71691f27dfb57387bb7b7ad5536eab7b02ebfc2a55f3a0e514b9b436f14ad1dc\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n","Collecting openai==0.28\n","  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Installing collected packages: openai\n","Successfully installed openai-0.28.0\n","Collecting prv_accountant\n","  Using cached prv_accountant-0.2.0-py3-none-any.whl (21 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from prv_accountant) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from prv_accountant) (1.11.4)\n","Installing collected packages: prv_accountant\n","Successfully installed prv_accountant-0.2.0\n"]}],"source":["\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import numpy as np\n","# !pip install array-to-latex\n","\n","from os.path import exists\n","import pandas as pd\n","\n","!pip install evaluate\n","!pip install rouge_score\n","!pip install openai==0.28\n","!pip install prv_accountant\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","path = \"/content/drive/Shareddrives/DP_ICL_Public\" # path for all files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Osm7RBe_3dLF","executionInfo":{"status":"ok","timestamp":1710032281354,"user_tz":300,"elapsed":2063,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"9ccd84e7-b98a-4cad-c34a-94038b7d58f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["# Cosine Similarity\n"],"metadata":{"id":"3Iz1zKpAV02F"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2367,"status":"ok","timestamp":1710031902311,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"},"user_tz":300},"id":"JhqaiYS3md2j","outputId":"37eee5c7-af63-4132-b8dc-4d96af93c58a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Edson is booking his ticket now.\n","\n","Joyce shared a link and Michael commented that it was a good deal, prompting Edson to quickly book a ticket.\n"," Joyce shared a link with Michael and Edson, who found it to be a good deal and decided to book a ticket.\n"]}],"source":["import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","label_path = path + \"100_test_truth.json\"\n","zero_shot_path = path + \"gpt3da-0shot-100ensemble-100test2div.json\"\n","four_shot_path = path + \"gpt3da-4shot-100ensemble-100test2.json\"\n","\n","with open(label_path, 'r') as f:\n","  label = json.load(f)\n","with open(zero_shot_path, 'r') as f:\n","  zero_pred_f = json.load(f)\n","zero_pred = []\n","for i in range(len(zero_pred_f)):\n","  zero_pred.append(zero_pred_f[str(i)]['prediction'])\n","with open(four_shot_path, 'r') as f:\n","  four_pred_f = json.load(f)\n","four_pred = []\n","for i in range(len(four_pred_f)):\n","  four_pred.append(four_pred_f[str(i)]['prediction'])\n","print(label[0])\n","print(zero_pred[0])\n","print(four_pred[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["bd0ed318ed854ff69dc31b2d235b8366","17c2c507c3ae4c529ef998dd3fe6bdf3","a5d4e826021b4bd68586529a8ef94d75","e404dffdbb664ee8bbef52c380419145","6cbd610b9f694f14b1db0da31707fb26","576770ae31e3431ca75d8f5cd9af56bb","27c46095eadc48b8b085cda1df5dd933","daa05e2310af435b9c209751327e8864","670861f02e3f4e6a9ce4953c6bca1123","8af23c567e51424eaa16f03003e23bb8","d956fe4869d349ebaa4907ff6af41a8d"]},"executionInfo":{"elapsed":49062,"status":"ok","timestamp":1710031976671,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"},"user_tz":300},"id":"2Ucu1bi9oWWb","outputId":"9a7223d6-1c76-4385-dfbf-e9cf21d1e243"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0ed318ed854ff69dc31b2d235b8366"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["zero-shot score: {'rouge1': 0.34347216497969557, 'rouge2': 0.11688048788686325, 'rougeL': 0.26272165873735354, 'rougeLsum': 0.2627416056610946}\n","four-shot score: {'rouge1': 0.43330772548421037, 'rouge2': 0.1906731330309419, 'rougeL': 0.347897654907557, 'rougeLsum': 0.34787852237585815}\n"]}],"source":["import evaluate\n","metric = evaluate.load(\"rouge\")\n","\n","def change_refer_pred(dr, en):\n","     data_references = []\n","     for data_refer in dr:\n","          for i in range(en):\n","               data_references.append(data_refer)\n","     return data_references\n","zero_pred_label = change_refer_pred(label, 100)\n","scores = metric.compute(predictions=zero_pred, references=zero_pred_label)\n","print(\"zero-shot score:\", scores)\n","\n","\n","four_pred_label = change_refer_pred(label, 100)\n","scores = metric.compute(predictions=four_pred, references=four_pred_label)\n","print(\"four-shot score:\", scores)\n"]},{"cell_type":"markdown","source":["### Nonprivate"],"metadata":{"id":"pKiiSBcm0f6f"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tSb0HASLq0uF","executionInfo":{"status":"ok","timestamp":1710032023601,"user_tz":300,"elapsed":15850,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"466852ac-b45b-4da9-d784-356687035832"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 1536) (10000, 1536)\n"," adj score {'rouge1': 0.4040835757939972, 'rouge2': 0.16506133308868248, 'rougeL': 0.3227834142771999, 'rougeLsum': 0.3235252374620169}\n"]}],"source":["embedding_four_shot = path + 'gpt3da-4shot-100ensemble-100test2eembedding.csv'\n","embedding_zero_shot = path + 'gpt3da-0shot-100ensemble-100test2divembeddingembedding.csv'\n","\n","indexs = []\n","zero_shot_embedding  = pd.read_csv(embedding_zero_shot).values\n","four_shot_embedding  = pd.read_csv(embedding_four_shot).values\n","print(zero_shot_embedding.shape, four_shot_embedding.shape)\n","\n","ensemble1 = 100\n","ensemble2 = 100\n","\n","for i in range(100):\n","     # get mean embedding of df2\n","     curr_emb = four_shot_embedding[i*ensemble2:(i+1)*ensemble2]\n","     curr_emb_sum = np.sum(curr_emb, axis=0, keepdims=True)\n","     mean_emb = curr_emb_sum/ensemble1 # (1, 1536) 1 data, 1536 features\n","     dist = cosine_similarity(mean_emb, zero_shot_embedding[i*ensemble1:(i+1)*ensemble1])\n","     indexs.append(np.argmax(dist)+i*ensemble2)\n"," #print(indexs)\n"," # get predictions from json file\n","final_pred = []\n","for index in indexs:\n","     final_pred.append(zero_pred[index])\n","\n","scores = metric.compute(predictions=final_pred, references=label)\n","print(\" adj score\", scores)\n","\n","\n","\n"]},{"cell_type":"markdown","source":["### private"],"metadata":{"id":"1LYX9_QY0kFH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgIARKBTsP0Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710032218212,"user_tz":300,"elapsed":72777,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"e492cdac-a393-4736-f506-221f362e1f49"},"outputs":[{"output_type":"stream","name":"stdout","text":["(10000, 1536) (10000, 1536)\n","noise 1.27186110977162\n","eps : 1\n","rouge1's mean:  0.3780967464645289 rouge1's std:  0.0030997026608231286\n","rouge2's mean:  0.14354214558656775 rouge2's std:  0.003739227247229012\n","rougeL's mean:  0.2950800721092415 rougeL's std:  0.005665949239475512\n","rougeLsum's mean:  0.29577568290356854 rougeLsum's std:  0.005875654934037727\n","noise 0.7870368587905289\n","eps : 3\n","rouge1's mean:  0.3912010755699068 rouge1's std:  0.005600481099570507\n","rouge2's mean:  0.15036945684834974 rouge2's std:  0.006627254751575122\n","rougeL's mean:  0.3072542844639169 rougeL's std:  0.004870206350579567\n","rougeLsum's mean:  0.3081601065364118 rougeLsum's std:  0.00486831399473413\n","noise 0.5481881634515281\n","eps : 8\n","rouge1's mean:  0.3973303902107222 rouge1's std:  0.00456113182905444\n","rouge2's mean:  0.15929606793622988 rouge2's std:  0.005081869790451159\n","rougeL's mean:  0.31348230027686186 rougeL's std:  0.003615679204142287\n","rougeLsum's mean:  0.3143403448678353 rougeLsum's std:  0.0037260658371122864\n"]}],"source":["embedding_four_shot = path + 'gpt3da-4shot-100ensemble-100test2eembedding.csv'\n","embedding_zero_shot = path + 'gpt3da-0shot-100ensemble-100test2divembeddingembedding.csv'\n","\n","\n","zero_shot_embedding  = pd.read_csv(embedding_zero_shot).values\n","four_shot_embedding  = pd.read_csv(embedding_four_shot).values\n","\n","\n","print(zero_shot_embedding.shape, four_shot_embedding.shape)\n","\n","from prv_accountant.dpsgd import find_noise_multiplier\n","size22 = 100\n","size11 = 100\n","num_step =100\n","for eps in [1,3,8]:\n","  sampling_probability = 100*4/14732 # 4 is 4-shot prediction, 14732 is the data size\n","  noise_multiplier = find_noise_multiplier(\n","                  sampling_probability=sampling_probability,\n","                  num_steps=num_step,\n","                  target_epsilon=eps,\n","                  target_delta=5e-5,\n","                  eps_error=0.01,\n","                  mu_max=100)\n","  print(\"noise\", noise_multiplier)\n","\n","  ensemble1 = 100\n","  ensemble2 = 100\n","  size1 = size11\n","  size2 = size22\n","  rouge1, rouge2, rougeL, rougeLsum = [],[],[],[]\n","  for _ in range(5):\n","    indexs = []\n","    for i in range(100):\n","        # get mean embedding of df2\n","        curr_emb = four_shot_embedding[(i*ensemble2):(i*ensemble2+size1)]\n","        curr_emb_sum = np.sum(curr_emb, axis=0, keepdims=True)\n","        curr_emb_sum += np.random.normal(loc=0, scale=noise_multiplier, size=curr_emb_sum.shape)\n","        mean_emb = curr_emb_sum/ensemble1 # (1, 1536) 1 data, 1536 features\n","        dist = cosine_similarity(mean_emb, zero_shot_embedding[(i*ensemble2):(i*ensemble2+size2)])\n","        indexs.append(np.argmax(dist)+i*ensemble2)\n","\n","    final_pred = []\n","    for index in indexs:\n","        final_pred.append(zero_pred[index])\n","\n","    score = metric.compute(predictions=final_pred, references=label)\n","\n","    rouge1.append(score['rouge1'])\n","    rouge2.append(score['rouge2'])\n","    rougeL.append(score['rougeL'])\n","    rougeLsum.append(score['rougeLsum'])\n","  print(\"eps :\", eps)\n","  print(\"rouge1's mean: \", np.mean(np.array(rouge1)), \"rouge1's std: \", np.std(np.array(rouge1)))\n","  print(\"rouge2's mean: \", np.mean(np.array(rouge2)), \"rouge2's std: \", np.std(np.array(rouge2)))\n","  print(\"rougeL's mean: \", np.mean(np.array(rougeL)), \"rougeL's std: \", np.std(np.array(rougeL)))\n","  print(\"rougeLsum's mean: \", np.mean(np.array(rougeLsum)), \"rougeLsum's std: \", np.std(np.array(rougeLsum)))\n"]},{"cell_type":"markdown","source":["# Key Word Extraction"],"metadata":{"id":"2IWIDXHXVtKG"}},{"cell_type":"code","source":["import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","label_path = path + \"100_test_truth.json\"\n","zero_shot_path = path + \"gpt3da-0shot-test100.json\"\n","four_shot_path = path + \"gpt3da-4shot-100ensemble-100test2.json\"\n","\n","with open(label_path, 'r') as f:\n","  label = json.load(f)\n","\n","with open(label_path, 'r') as f:\n","  label = json.load(f)\n","with open(zero_shot_path, 'r') as f:\n","  zero_pred_f = json.load(f)\n","zero_pred = []\n","for i in range(len(zero_pred_f)):\n","  zero_pred.append(zero_pred_f[str(i)]['prediction'])\n","with open(four_shot_path, 'r') as f:\n","  four_pred_f = json.load(f)\n","four_pred = []\n","for i in range(len(four_pred_f)):\n","  four_pred.append(four_pred_f[str(i)]['prediction'])\n","\n","fourshot_pred = four_pred\n","\n","import evaluate\n","metric = evaluate.load(\"rouge\")\n","\n","def change_refer_pred(dr, en):\n","     data_references = []\n","     for data_refer in dr:\n","          for i in range(en):\n","               data_references.append(data_refer)\n","     return data_references\n","zero_pred_label = change_refer_pred(label, 1)\n","scores = metric.compute(predictions=zero_pred, references=zero_pred_label)\n","print(\"zero-shot score:\", scores)\n","scores = metric.compute(predictions=four_pred, references=four_pred_label)\n","print(\"four-shot score:\", scores)\n","\n","\n","\n"],"metadata":{"id":"vwb2xQ_N5haT","executionInfo":{"status":"ok","timestamp":1710032871710,"user_tz":300,"elapsed":17002,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68ac5ada-2350-4bb7-f919-51377f36e8d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["zero-shot score: {'rouge1': 0.3532397885072065, 'rouge2': 0.12692127348949506, 'rougeL': 0.2706758874386449, 'rougeLsum': 0.2708054072085293}\n","four-shot score: {'rouge1': 0.4332034565999593, 'rouge2': 0.19079464272450541, 'rougeL': 0.3477672056620673, 'rougeLsum': 0.34786986933427066}\n"]}]},{"cell_type":"markdown","source":["## Key word without ranking"],"metadata":{"id":"FG3OTnLCQzTB"}},{"cell_type":"markdown","source":["### Non-private KSA without ranking\n"],"metadata":{"id":"4Sb-iSg-ucUk"}},{"cell_type":"code","source":["ds_size = 100 #test data size\n","repeat = 1\n","ensemble = 100\n","from tqdm import tqdm\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","import string\n","import random\n","import openai\n","import evaluate\n","metric = evaluate.load(\"rouge\")\n","\n","stopword_set = set(stopwords.words('english'))\n","\n","rouge1, rouge2, rougeL, rougeLsum = [],[],[],[]\n","for _ in range(repeat):\n","\n","     # get predictions from json file\n","     final_pred = []\n","     count_threshold_list = []\n","     for i in tqdm(range(ds_size)):\n","          all_tokens = {} # key: token, value: count\n","          for j in range(ensemble):\n","               sentence = fourshot_pred[i*ensemble+j]\n","               tokens = nltk.word_tokenize(sentence)\n","               onegrams = set(ngrams(tokens, 1))\n","               # onegrams = set(onegrams)\n","               # making onegrams a set to avoid duplicate tokens\n","               for token in onegrams:\n","                    # only add one gram for one sentence\n","                    if token in all_tokens:\n","                         all_tokens[token] += 1\n","                    else:\n","                         all_tokens[token] = 1\n","          #print(all_tokens)\n","          all_tokens_sorted = sorted(all_tokens.items(), key=lambda x: x[1], reverse=True)\n","          #print(all_tokens_sorted)\n","          # ignore those non-words tokens\n","          filtered_tokens = {}\n","          for token, count in all_tokens_sorted:\n","               if not all(word in string.punctuation for word in token) and token[0] not in stopword_set:\n","                    filtered_tokens[token] = count\n","          filtered_tokens_sorted = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n","          #print(filtered_tokens)\n","          # find the count threshold where the count gap is the largest\n","          actually_upper_bound = 0\n","          count_threshold = 0 # you need to creat a list to store all difference between the counts\n","          for k in range(len(filtered_tokens_sorted)-1):\n","               #print(k,len(filtered_tokens_sorted)-2)\n","               if filtered_tokens_sorted[k][1] - filtered_tokens_sorted[k+1][1] > count_threshold:\n","                    count_threshold = filtered_tokens_sorted[k][1] - filtered_tokens_sorted[k+1][1]\n","                    actually_upper_bound = filtered_tokens_sorted[k][1]\n","               if k == len(filtered_tokens_sorted)-2:\n","                    if  filtered_tokens_sorted[k+1][1] > count_threshold:\n","                         count_threshold = filtered_tokens_sorted[k+1][1]\n","                         actually_upper_bound =   filtered_tokens_sorted[k+1][1] #including all tokens\n","\n","          #print(count_threshold)\n","          filtered_tokens = dict(filtered_tokens_sorted)\n","          filtered_tokens = [k[0] for k, v in filtered_tokens.items() if v >= actually_upper_bound]\n","          #print(filtered_tokens)\n","\n","          random.shuffle(filtered_tokens) #shuffle the list of tokens\n","          prompt = '['\n","          for token in filtered_tokens:\n","               prompt += token + ', '\n","          prompt = prompt[:-2] + ']'\n","          prompt = prompt + '\\nThe summary is:'\n","          zero_shot_sentence = zero_pred_f[str(i)]['origin_prompt'].replace('Summarize the above dialogue:','Summarize the above dialogue with the following word suggestions:')\n","          #print(zero_shot_sentence)\n","          final_prompt = zero_shot_sentence  + prompt\n","          # if i < 5 :\n","          #   print(final_prompt, label[i])\n","\n","          ################################################################################\n","          ##### Please check your code and run this api to complete the experiment'#######\n","          ################################################################################\n","\n","          openai.api_key = \"\"\n","          pred = openai.Completion.create(\n","               engine=\"text-davinci-003\",\n","               prompt=final_prompt,\n","               temperature=0,\n","               max_tokens=256,\n","               top_p=1.0,\n","               frequency_penalty=0.0,\n","               presence_penalty=0.0\n","          )\n","          final_pred.append(pred['choices'][0]['text'])\n","\n","     score = metric.compute(predictions=final_pred, references=label)\n","     rouge1.append(score['rouge1'])\n","     rouge2.append(score['rouge2'])\n","     rougeL.append(score['rougeL'])\n","     rougeLsum.append(score['rougeLsum'])\n","print(\"rouge1's mean: \", np.mean(np.array(rouge1)), \"rouge1's std: \", np.std(np.array(rouge1)))\n","print(\"rouge2's mean: \", np.mean(np.array(rouge2)), \"rouge2's std: \", np.std(np.array(rouge2)))\n","print(\"rougeL's mean: \", np.mean(np.array(rougeL)), \"rougeL's std: \", np.std(np.array(rougeL)))\n","print(\"rougeLsum's mean: \", np.mean(np.array(rougeLsum)), \"rougeLsum's std: \", np.std(np.array(rougeLsum)))\n"],"metadata":{"id":"aEo4mPu4Xjn7","colab":{"base_uri":"https://localhost:8080/","height":478},"executionInfo":{"status":"error","timestamp":1710032876382,"user_tz":300,"elapsed":685,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"e23db3e2-f7ba-4cf8-bbbf-b858c0d2b484"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","  0%|          | 0/100 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"AuthenticationError","evalue":"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-2c696c93fb94>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m           \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m           pred = openai.Completion.create(\n\u001b[0m\u001b[1;32m     84\u001b[0m                \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n","\u001b[0;31mAuthenticationError\u001b[0m: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."]}]},{"cell_type":"markdown","source":["### compute dp parameter"],"metadata":{"id":"XptXjWVdvQ1y"}},{"cell_type":"code","source":["!pip install autodp\n","from autodp.autodp_core import Mechanism\n","from autodp.transformer_zoo import Composition\n","from autodp import mechanism_zoo, transformer_zoo\n","from autodp import rdp_bank\n","from autodp import rdp_acct\n","from autodp import utils\n","\n","import math\n","\n","from autodp.privacy_calibrator import subsample_epsdelta\n","\n","from scipy.optimize import minimize_scalar\n","\n","import numpy as np\n","def rdp_to_approxdp(rdp, alpha_max=np.inf, BBGHS_conversion=True):\n","    # from RDP to approx DP\n","    # alpha_max is an optional input which sometimes helps avoid numerical issues\n","    # By default, we are using the RDP to approx-DP conversion due to BBGHS'19's Theorem 21\n","    # paper: https://arxiv.org/pdf/1905.09982.pdf\n","    # if you need to use the simpler RDP to approxDP conversion for some reason, turn the flag off\n","\n","    def approxdp(delta):\n","\n","        \"\"\"\n","        approxdp outputs eps as a function of delta based on rdp calculations\n","        :param delta:\n","        :return: the \\epsilon with a given delta\n","        \"\"\"\n","\n","        if delta < 0 or delta > 1:\n","            print(\"Error! delta is a probability and must be between 0 and 1\")\n","        if delta == 0:\n","            return rdp(np.inf)\n","        else:\n","            def fun(x):  # the input the RDP's alpha\n","                if x <= 1:\n","                    return np.inf\n","                else:\n","                    if BBGHS_conversion:\n","                        return np.maximum(rdp(x) + np.log((x-1)/x) - (np.log(delta) + np.log(x))/(x-1), 0)\n","                    else:\n","                        return np.log(1 / delta) / (x - 1) + rdp(x)\n","\n","            results = minimize_scalar(fun, method='Bounded', bracket=(1,2), bounds=(1, alpha_max) )\n","            if results.success:\n","                return results.fun\n","            else:\n","                # There are cases when certain \\delta is not feasible.\n","                # For example, let p and q be uniform the privacy R.V. is either 0 or \\infty and unless all \\infty\n","                # events are taken cared of by \\delta, \\epsilon cannot be < \\infty\n","                return np.inf\n","    return approxdp\n","\n","\n","def approxRDP_to_approxDP(delta, delta0, rdp_func, alpha_max=np.inf, BBGHS_conversion=True):\n","\n","  if delta < delta0:\n","    return np.inf\n","\n","  delta1 = delta - delta0\n","\n","  approxdp = rdp_to_approxdp(rdp_func, alpha_max, BBGHS_conversion)\n","\n","  return approxdp(delta1)\n","\n","\n","\n","# eps is the privacy parameter for EM\n","# delta0 is the failure probability for PTR\n","class EMandPTR(Mechanism):\n","  def __init__(self, eps, delta0, sigma, name='PTR'):\n","    Mechanism.__init__(self)\n","    self.name=name\n","    self.params={'delta0':delta0, 'sigma':sigma}\n","\n","    def privloss(t, alpha):\n","      return (np.exp(alpha*(eps-t)) - np.exp(-alpha*t) - (np.exp(alpha*eps-(alpha+1)*t) - np.exp(eps-(alpha+1)*t))) / ( np.exp(eps-t) - np.exp(-t) )\n","\n","\n","\n","    def RDP_EM(alpha):\n","      temp = (np.sinh(alpha*eps) - np.sinh((alpha-1)*eps)) / np.sinh(eps)\n","      return min( 1/2 * alpha * eps**2, np.log(temp) / (alpha-1) )\n","\n","    def approxRDP_PTR(alpha, delta0):\n","      return rdp_bank.RDP_gaussian({'sigma': sigma}, alpha)\n","\n","    def approxRDP_EMandPTR(alpha, delta0):\n","      return rdp_bank.RDP_gaussian({'sigma': sigma}, alpha) + RDP_EM(alpha)\n","\n","    def fakeRDP_EMandPTR(alpha):\n","      if alpha == np.inf:\n","        return np.inf\n","      return approxRDP_EMandPTR(alpha, delta0)\n","\n","    self.propagate_updates(fakeRDP_EMandPTR, 'RDP')\n","\n","\n","class compose_subsampled_EMandPTR(Mechanism):\n","  def __init__(self, eps, delta0, sigma, prob, niter, name='compose_subsampled_EMandPTR'):\n","\n","    Mechanism.__init__(self)\n","    self.name=name\n","\n","    subsample = transformer_zoo.AmplificationBySampling() # by default this is using poisson sampling\n","    compose = transformer_zoo.Composition()\n","\n","    mech = EMandPTR(eps, delta0, sigma)\n","    mech.neighboring = 'add_remove'\n","\n","    if prob < 1:\n","      mech = subsample(mech, prob*(1-delta0) / (1-prob*delta0), improved_bound_flag=False)\n","\n","    mech = compose([mech], [niter])\n","    rdp_total = mech.RenyiDP\n","\n","    self.propagate_updates(rdp_total, type_of_update='RDP')\n","\n","\n","# params = {eps, delta0, sigma, prob, niter}\n","def compose_subsampled_EMandPTR_to_approxDP(params, delta):\n","\n","  mech = compose_subsampled_EMandPTR(params['eps'], params['delta0'], params['sigma'], params['prob'], params['niter'])\n","  rdp_func = mech.RenyiDP\n","\n","  return approxRDP_to_approxDP(delta, params['delta0']*params['prob'], rdp_func, alpha_max=200, BBGHS_conversion=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UBI80Aq8ywSn","executionInfo":{"status":"ok","timestamp":1710032888595,"user_tz":300,"elapsed":8820,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"cbed3c25-508a-4df8-a864-88a720a65716"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting autodp\n","  Downloading autodp-0.2.3.1.tar.gz (56 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from autodp) (1.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autodp) (3.7.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from autodp) (1.25.2)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from autodp) (1.11.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->autodp) (1.16.0)\n","Building wheels for collected packages: autodp\n","  Building wheel for autodp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autodp: filename=autodp-0.2.3.1-py3-none-any.whl size=59816 sha256=79ee4895a47cc24928bc6bf61dde2bcfc3a56e31dce9e3f3fd26b516fc41c54c\n","  Stored in directory: /root/.cache/pip/wheels/bb/fa/09/a4bf92616f2b3353c47e75d36c14afc2aefce2b78ad6e3536f\n","Successfully built autodp\n","Installing collected packages: autodp\n","Successfully installed autodp-0.2.3.1\n"]}]},{"cell_type":"code","source":["delta = 5e-5\n","prob = 400/14732\n","# here you need to check and let the output of compose_subsampled_EMandPTR_to_approxDP\n","# to be less the your target epsilon\n","\n","\n","# privacy_params = {\n","#     'eps': 1,\n","#     'delta0': 0.99*delta/prob,\n","#     'sigma': 1.15046,\n","#     'prob': prob,\n","#     'niter': 100\n","# } #91\n","\n","print(delta/prob)\n","\n","\n","\n","privacy_params = {\n","    'eps': 0.9,\n","    'delta0': 0.99*delta/prob,\n","    'sigma': 1.869,\n","    'prob': prob,\n","    'niter': 100\n","} # 79\n","\n","# privacy_params = {\n","#     'eps': 0.45,\n","#     'delta0': 0.8*delta/prob,\n","#     'sigma': 2.62,\n","#     'prob': prob,\n","#     'niter': 100\n","# } # 57\n","\n","\n","print(privacy_params['delta0'],privacy_params['eps'])\n","compose_subsampled_EMandPTR_to_approxDP(privacy_params, delta=delta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BsrcBgsryPG7","executionInfo":{"status":"ok","timestamp":1710032892099,"user_tz":300,"elapsed":296,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"555723a9-bb82-4d79-c74b-3e182c217eba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0018415\n","0.0018230850000000001 0.9\n"]},{"output_type":"execute_result","data":{"text/plain":["2.9995872921472246"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["### private KSA with PTR"],"metadata":{"id":"jqzd8orHvYro"}},{"cell_type":"code","source":["\n","ds_size = 100 #test data size\n","repeat = 1\n","ensemble = 100\n","from tqdm import tqdm\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","import string\n","import random\n","import openai\n","import evaluate\n","metric = evaluate.load(\"rouge\")\n","np.random.seed(123)\n","random.seed(123)\n","stopword_set = set(stopwords.words('english'))\n","\n","count_pass = 0\n","gap_change = []\n","print(len(fourshot_pred))\n","rouge1, rouge2, rougeL, rougeLsum = [],[],[],[]\n","for _ in range(repeat):\n","\n","     # get predictions from json file\n","     final_pred = []\n","     count_threshold_list = []\n","     for i in range(ds_size):\n","          all_tokens = {} # key: token, value: count\n","          for j in range(ensemble):\n","               sentence = fourshot_pred[i*ensemble+j]\n","               tokens = nltk.word_tokenize(sentence)\n","               onegrams = set(ngrams(tokens, 1))\n","               # onegrams = set(onegrams)\n","               # making onegrams a set to avoid duplicate tokens\n","               for token in onegrams:\n","                    # only add one gram for one sentence\n","                    if token in all_tokens:\n","                         all_tokens[token] += 1\n","                    else:\n","                         all_tokens[token] = 1\n","          #print(all_tokens)\n","          all_tokens_sorted = sorted(all_tokens.items(), key=lambda x: x[1], reverse=True)\n","          #print(all_tokens_sorted)\n","          # ignore those non-words tokens\n","          filtered_tokens = {}\n","          for token, count in all_tokens_sorted:\n","               if not all(word in string.punctuation for word in token) and token[0] not in stopword_set:\n","                    filtered_tokens[token] = count\n","          filtered_tokens_sorted = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n","          #print(filtered_tokens)\n","\n","          ### DP version (RNM)\n","          gap_lst = []\n","          for k in range( min(len(filtered_tokens_sorted)-1, 30) ):   # I assume we are only interested in k <= 30\n","            gap = filtered_tokens_sorted[k][1] - filtered_tokens_sorted[k+1][1]\n","            gap_lst.append(gap)\n","            if k == len(filtered_tokens_sorted)-2:\n","              gap_lst.append(filtered_tokens_sorted[k+1][1])\n","\n","          gap_lst = np.array(gap_lst)\n","          gap_max_prev = np.max(gap_lst)\n","          noisy_gap_lst = gap_lst + np.random.gumbel(0, 2/privacy_params['eps'], len(gap_lst))\n","          kstar = np.argmax(noisy_gap_lst)\n","          gap_max = np.max(noisy_gap_lst)\n","          gap_change.append(gap_max_prev-gap_max)\n","\n","\n","          #print(count_threshold)\n","          filtered_tokens = dict(filtered_tokens_sorted)\n","          print(filtered_tokens)\n","\n","          ### Non-private\n","          #filtered_tokens = [k[0] for k, v in filtered_tokens.items() if v >= actually_upper_bound]\n","\n","          ### DP version (PTR)\n","          from scipy import stats\n","          dk = gap_lst[kstar]\n","          noise1 = np.random.normal(0, 2*privacy_params['sigma'])\n","          noise2 = stats.norm.isf(privacy_params['delta0'], loc=0, scale=2*privacy_params['sigma'])\n","          dkhat = max(2, dk) + noise1 - noise2\n","          print('dk={}, noise1={}, noise2={}, dkhat={}'.format(dk, noise1, noise2, dkhat))\n","          if dkhat > 2:\n","            print(\"label\", len(label[i]),\"zero\", len(zero_pred_f[str(i)]['prediction']),\"four\", len(fourshot_pred[i*ensemble+0]))\n","            print('***PASS TEST! RELEASE EXACT TOP-{} TOKENS***'.format(kstar))\n","            filtered_tokens = [k[0] for k, v in filtered_tokens.items()]\n","            filtered_tokens = filtered_tokens[:kstar]\n","            count_pass += 1\n","          else:\n","            print('***FAIL TEST! DO Zero-shot Learning***'.format(kstar))\n","            filtered_tokens = []\n","            print(\"label\", len(label[i]),\"zero\", len(zero_pred_f[str(i)]['prediction']),\"four\", len(fourshot_pred[i*ensemble+0]))\n","\n","          # print(filtered_tokens)\n","          if filtered_tokens == []:\n","            final_prompt = zero_pred_f[str(i)]['origin_prompt']\n","          else:\n","            random.shuffle(filtered_tokens) #shuffle the list of tokens\n","            prompt = '['\n","            for token in filtered_tokens:\n","                prompt += token + ', '\n","            prompt = prompt[:-2] + ']'\n","            prompt = prompt + '\\nThe summary is:'\n","            zero_shot_sentence = zero_pred_f[str(i)]['origin_prompt'].replace('Summarize the above dialogue:','Summarize the above dialogue with the following word suggestions:')\n","            #print(zero_shot_sentence)\n","            final_prompt = zero_shot_sentence  + prompt\n","          if i < 5 :\n","            print(final_prompt)\n","\n","          ################################################################################\n","          ##### Please check your code and run this api to complete the experiment'#######\n","          ################################################################################\n","\n","          pred = openai.Completion.create(\n","               engine=\"text-davinci-003\",\n","               prompt=final_prompt,\n","               temperature=0,\n","               max_tokens=256,\n","               top_p=1.0,\n","               frequency_penalty=0.0,\n","               presence_penalty=0.0\n","          )\n","          final_pred.append(pred['choices'][0]['text'])\n","\n","\n","\n","     score = metric.compute(predictions=final_pred, references=label)\n","     rouge1.append(score['rouge1'])\n","     rouge2.append(score['rouge2'])\n","     rougeL.append(score['rougeL'])\n","     rougeLsum.append(score['rougeLsum'])\n","print(\"rouge1's mean: \", np.mean(np.array(rouge1)), \"rouge1's std: \", np.std(np.array(rouge1)))\n","print(\"rouge2's mean: \", np.mean(np.array(rouge2)), \"rouge2's std: \", np.std(np.array(rouge2)))\n","print(\"rougeL's mean: \", np.mean(np.array(rougeL)), \"rougeL's std: \", np.std(np.array(rougeL)))\n","print(\"rougeLsum's mean: \", np.mean(np.array(rougeLsum)), \"rougeLsum's std: \", np.std(np.array(rougeLsum)))\n","\n","print(count_pass)\n","print(np.mean(np.array(gap_change)))\n","\n","\n","# ***PASS TEST! RELEASE EXACT TOP-3 TOKENS***\n","# rouge1's mean:  0.38549113959158926 rouge1's std:  0.004715646681797527\n","# rouge2's mean:  0.1442157014406559 rouge2's std:  0.005443081525002633\n","# rougeL's mean:  0.2958618078347388 rougeL's std:  0.0045467990552312655\n","# rougeLsum's mean:  0.29607762247992897 rougeLsum's std:  0.0043788640926342645\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":706},"id":"1-FDaL9Zisja","executionInfo":{"status":"error","timestamp":1710032898983,"user_tz":300,"elapsed":1081,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"a8eef1cc-8db8-4423-9db2-31e376fe9e83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["10000\n","{('Joyce',): 100, ('link',): 100, ('Edson',): 100, ('shared',): 96, ('Michael',): 93, ('ticket',): 93, ('cheap',): 50, ('good',): 48, ('deal',): 47, ('booking',): 46, ('book',): 42, ('found',): 41, ('decided',): 39, ('excited',): 24, ('away',): 19, ('right',): 19, ('commented',): 13, ('said',): 11, ('thought',): 10, ('think',): 6, ('They',): 5, ('showed',): 4, ('flight',): 3, ('find',): 3, ('surprised',): 3, (\"'s\",): 2, ('impressed',): 2, ('price',): 2, ('noticed',): 2, ('offer',): 1, ('deciding',): 1}\n","dk=43, noise1=-6.936372847080687, noise2=10.867315459438492, dkhat=25.196311693480816\n","label 32 zero 132 four 105\n","***PASS TEST! RELEASE EXACT TOP-5 TOKENS***\n","\n","Dialogue:\n"," Joyce: Check this out!\r\n","Joyce: <link>\r\n","Michael: That's cheap!\r\n","Edson: No way! I'm booking my ticket now!!  \n","Summarize the above dialogue with the following word suggestions: [shared, link, Michael, Edson, Joyce]\n","The summary is:\n"]},{"output_type":"error","ename":"AuthenticationError","evalue":"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-79229736d222>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m           \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m           pred = openai.Completion.create(\n\u001b[0m\u001b[1;32m    115\u001b[0m                \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n","\u001b[0;31mAuthenticationError\u001b[0m: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."]}]},{"cell_type":"markdown","source":["## Key word with ranking"],"metadata":{"id":"bftPROM0Q75h"}},{"cell_type":"code","source":["import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","label_path = path + \"100_test_truth.json\"\n","zero_shot_path = path + \"gpt3da-0shot-100ensemble-100test2div.json\"\n","\n","\n","with open(zero_shot_path, 'r') as f:\n","  zero_pred_f2 = json.load(f)\n","zero_pred2 = []\n","for i in range(len(zero_pred_f2)):\n","  zero_pred2.append(zero_pred_f2[str(i)]['prediction'])\n","\n","\n","fourshot_pred = zero_pred2\n","print(len(fourshot_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqLF2k3or8uD","executionInfo":{"status":"ok","timestamp":1710032945253,"user_tz":300,"elapsed":1280,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"a1aa38aa-f142-4ff9-a21b-5b55a5eaf14a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10000\n"]}]},{"cell_type":"markdown","source":["### Non-private KSA with ranking"],"metadata":{"id":"rQG7XvtIv328"}},{"cell_type":"code","source":["ds_size = 100 #test data size\n","repeat = 1\n","ensemble = 100\n","kw_size = 10\n","from tqdm import tqdm\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","import string\n","import random\n","import openai\n","import evaluate\n","metric = evaluate.load(\"rouge\")\n","\n","stopword_set = set(stopwords.words('english'))\n","\n","rouge1, rouge2, rougeL, rougeLsum = [],[],[],[]\n","for _ in range(repeat):\n","\n","     # get predictions from json file\n","     final_pred = []\n","     count_threshold_list = []\n","     for i in tqdm(range(ds_size)):\n","          all_tokens = {} # key: token, value: count\n","          for j in range(kw_size):\n","               sentence = fourshot_pred[i*ensemble+j]\n","               tokens = nltk.word_tokenize(sentence)\n","               onegrams = set(ngrams(tokens, 1))\n","               # onegrams = set(onegrams)\n","               # making onegrams a set to avoid duplicate tokens\n","               for token in onegrams:\n","                    # only add one gram for one sentence\n","                    if token in all_tokens:\n","                         all_tokens[token] += 1\n","                    else:\n","                         all_tokens[token] = 1\n","          #print(all_tokens)\n","          all_tokens_sorted = sorted(all_tokens.items(), key=lambda x: x[1], reverse=True)\n","          #print(all_tokens_sorted)\n","          # ignore those non-words tokens\n","          filtered_tokens = {}\n","          for token, count in all_tokens_sorted:\n","               if not all(word in string.punctuation for word in token) and token[0] not in stopword_set:\n","                    filtered_tokens[token] = count\n","          filtered_tokens_sorted = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n","          #print(filtered_tokens)\n","          # find the count threshold where the count gap is the largest\n","          actually_upper_bound = 0\n","          count_threshold = 0 # you need to creat a list to store all difference between the counts\n","          for k in range(len(filtered_tokens_sorted)-1):\n","               #print(k,len(filtered_tokens_sorted)-2)\n","               if filtered_tokens_sorted[k][1] - filtered_tokens_sorted[k+1][1] > count_threshold:\n","                    count_threshold = filtered_tokens_sorted[k][1] - filtered_tokens_sorted[k+1][1]\n","                    actually_upper_bound = filtered_tokens_sorted[k][1]\n","               if k == len(filtered_tokens_sorted)-2:\n","                    if  filtered_tokens_sorted[k+1][1] > count_threshold:\n","                         count_threshold = filtered_tokens_sorted[k+1][1]\n","                         actually_upper_bound =   filtered_tokens_sorted[k+1][1] #including all tokens\n","\n","          #print(count_threshold)\n","          filtered_tokens_new = {}\n","          for k, v in filtered_tokens.items():\n","            if v >= actually_upper_bound:\n","              if filtered_tokens_new.get(v) is None:\n","                filtered_tokens_new[v] = [k[0]]\n","              else:\n","                filtered_tokens_new[v].append(k[0])\n","\n","          #print(filtered_tokens_new)\n","          for key in filtered_tokens_new:\n","              # Shuffle the elements within the key\n","              random.shuffle(filtered_tokens_new[key])\n","          #print(filtered_tokens_new)\n","          filtered_tokens = []\n","          for key in filtered_tokens_new:\n","              # Shuffle the elements within the key\n","              filtered_tokens+=filtered_tokens_new[key]\n","          print(filtered_tokens)\n","          prompt = '['\n","          for token in filtered_tokens:\n","               prompt += token + ', '\n","          prompt = prompt[:-2] + ']'\n","          prompt = prompt + '\\nThe summary is:'\n","          zero_shot_sentence = zero_pred_f[str(i)]['origin_prompt'].replace('Summarize the above dialogue:','Summarize the above dialogue with the following word suggestions ranked by their frequency from high to low:')\n","          #print(zero_shot_sentence)\n","          final_prompt = zero_shot_sentence  + prompt\n","          if i < 5 :\n","            print(final_prompt, label[i])\n","\n","          ################################################################################\n","          ##### Please check your code and run this api to complete the experiment'#######\n","          ################################################################################\n","\n","          openai.api_key = \"\"\n","          pred = openai.Completion.create(\n","               engine=\"text-davinci-003\",\n","               prompt=final_prompt,\n","               temperature=0,\n","               max_tokens=256,\n","               top_p=1.0,\n","               frequency_penalty=0.0,\n","               presence_penalty=0.0\n","          )\n","          final_pred.append(pred['choices'][0]['text'])\n","\n","          # final_pred.append(\"test\") # comment this to run.\n","\n","          # print(final_pred[-1])\n","\n","\n","     score = metric.compute(predictions=final_pred, references=label)\n","     rouge1.append(score['rouge1'])\n","     rouge2.append(score['rouge2'])\n","     rougeL.append(score['rougeL'])\n","     rougeLsum.append(score['rougeLsum'])\n","print(\"rouge1's mean: \", np.mean(np.array(rouge1)), \"rouge1's std: \", np.std(np.array(rouge1)))\n","print(\"rouge2's mean: \", np.mean(np.array(rouge2)), \"rouge2's std: \", np.std(np.array(rouge2)))\n","print(\"rougeL's mean: \", np.mean(np.array(rougeL)), \"rougeL's std: \", np.std(np.array(rougeL)))\n","print(\"rougeLsum's mean: \", np.mean(np.array(rougeLsum)), \"rougeLsum's std: \", np.std(np.array(rougeLsum)))\n"],"metadata":{"id":"K8VjTyDeijg-","colab":{"base_uri":"https://localhost:8080/","height":654},"executionInfo":{"status":"error","timestamp":1710032949995,"user_tz":300,"elapsed":626,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"61e20daa-1e0e-4c7e-a856-0d8ea42f1343"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","  0%|          | 0/100 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["['Edson', 'ticket', 'Joyce', 'book', 'link', 'Michael', 'decided']\n","\n","Dialogue:\n"," Joyce: Check this out!\r\n","Joyce: <link>\r\n","Michael: That's cheap!\r\n","Edson: No way! I'm booking my ticket now!!  \n","Summarize the above dialogue with the following word suggestions ranked by their frequency from high to low: [Edson, ticket, Joyce, book, link, Michael, decided]\n","The summary is: Edson is booking his ticket now.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"AuthenticationError","evalue":"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-d8999e6b8c9d>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m           \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m           pred = openai.Completion.create(\n\u001b[0m\u001b[1;32m     98\u001b[0m                \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n","\u001b[0;31mAuthenticationError\u001b[0m: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys."]}]},{"cell_type":"markdown","source":["### Exponential Mechanism"],"metadata":{"id":"VCVSpDPQyVCI"}},{"cell_type":"code","source":["!pip install autodp\n","from autodp.autodp_core import Mechanism\n","from autodp.transformer_zoo import Composition\n","from autodp import mechanism_zoo, transformer_zoo\n","from autodp import rdp_bank\n","from autodp import rdp_acct\n","from autodp import utils\n","\n","import math\n","\n","from autodp.privacy_calibrator import subsample_epsdelta\n","\n","from scipy.optimize import minimize_scalar\n","\n","import numpy as np"],"metadata":{"id":"MQb41PJ26qsh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710032958473,"user_tz":300,"elapsed":6465,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"2dc17199-b259-4402-a1ed-20dad88ea332"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: autodp in /usr/local/lib/python3.10/dist-packages (0.2.3.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from autodp) (1.4.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autodp) (3.7.1)\n","Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from autodp) (1.25.2)\n","Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from autodp) (1.11.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->autodp) (1.16.0)\n"]}]},{"cell_type":"code","source":["def rdp_to_approxdp(rdp, alpha_max=np.inf, BBGHS_conversion=True):\n","    # from RDP to approx DP\n","    # alpha_max is an optional input which sometimes helps avoid numerical issues\n","    # By default, we are using the RDP to approx-DP conversion due to BBGHS'19's Theorem 21\n","    # paper: https://arxiv.org/pdf/1905.09982.pdf\n","    # if you need to use the simpler RDP to approxDP conversion for some reason, turn the flag off\n","\n","    def approxdp(delta):\n","\n","        \"\"\"\n","        approxdp outputs eps as a function of delta based on rdp calculations\n","        :param delta:\n","        :return: the \\epsilon with a given delta\n","        \"\"\"\n","\n","        if delta < 0 or delta > 1:\n","            print(\"Error! delta is a probability and must be between 0 and 1\")\n","        if delta == 0:\n","            return rdp(np.inf)\n","        else:\n","            def fun(x):  # the input the RDP's alpha\n","                if x <= 1:\n","                    return np.inf\n","                else:\n","                    if BBGHS_conversion:\n","                        return np.maximum(rdp(x) + np.log((x-1)/x) - (np.log(delta) + np.log(x))/(x-1), 0)\n","                    else:\n","                        return np.log(1 / delta) / (x - 1) + rdp(x)\n","\n","            results = np.min( [fun(alpha) for alpha in range(1, alpha_max)] )\n","            return results\n","\n","            # results = minimize_scalar(fun, method='Bounded', bounds=(1, alpha_max) )\n","            # if results.success:\n","            #     return results.fun\n","            # else:\n","            #     # There are cases when certain \\delta is not feasible.\n","            #     # For example, let p and q be uniform the privacy R.V. is either 0 or \\infty and unless all \\infty\n","            #     # events are taken cared of by \\delta, \\epsilon cannot be < \\infty\n","            #     return np.inf\n","    return approxdp\n","\n","\n","def approxRDP_to_approxDP(delta, delta0, rdp_func, alpha_max=np.inf, BBGHS_conversion=True):\n","\n","  if delta < delta0:\n","    return np.inf\n","\n","  delta1 = delta - delta0\n","\n","  approxdp = rdp_to_approxdp(rdp_func, alpha_max, BBGHS_conversion)\n","\n","  return approxdp(delta1)"],"metadata":{"id":"FuFU_0-QCKpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# eps is the privacy parameter for EM\n","# add Gumbel(sensitivity/eps), where sensitivity is the sensitivity of utility function\n","class EM(Mechanism):\n","  def __init__(self, eps, name='EM', monotone=False):\n","    Mechanism.__init__(self)\n","    self.name=name\n","    self.params={'eps':eps}\n","\n","    if monotone:\n","\n","      def privloss(t, alpha):\n","        return (np.exp(alpha*(eps-t)) - np.exp(-alpha*t) - (np.exp(alpha*eps-(alpha+1)*t) - np.exp(eps-(alpha+1)*t))) / ( np.exp(eps-t) - np.exp(-t) )\n","\n","      def RDP_EM(alpha):\n","        if alpha == np.infty:\n","          return eps\n","        enegt = ((alpha-1)*(np.exp(alpha*eps)-1)) / ((alpha)*(np.exp(alpha*eps)-np.exp(eps)))\n","        return np.log( privloss(np.log(1/enegt), alpha) ) / (alpha-1)\n","\n","    else:\n","\n","      def RDP_EM(alpha):\n","        if alpha == np.infty:\n","          return eps*2\n","        temp = (np.sinh(alpha*eps) - np.sinh((alpha-1)*eps)) / np.sinh(eps)\n","        return min( 1/2 * alpha * eps**2, np.log(temp) / (alpha-1) )\n","\n","    self.propagate_updates(RDP_EM, 'RDP')\n","\n","\n","class compose_subsampled_EM(Mechanism):\n","  def __init__(self, eps, prob, niter, name='compose_subsampled_EM', monotone=False):\n","\n","    Mechanism.__init__(self)\n","    self.name=name\n","\n","    subsample = transformer_zoo.AmplificationBySampling() # by default this is using poisson sampling\n","    compose = transformer_zoo.Composition()\n","\n","    mech = EM(eps, monotone=monotone)\n","    mech.neighboring = 'add_remove'\n","\n","    if prob < 1:\n","      mech = subsample(mech, prob, improved_bound_flag=False)\n","\n","    mech = compose([mech], [niter])\n","    rdp_total = mech.RenyiDP\n","\n","    self.propagate_updates(rdp_total, type_of_update='RDP')\n","\n","\n","# params = {eps, prob, niter}\n","def compose_subsampled_EM_to_approxDP(params, delta):\n","\n","  mech = compose_subsampled_EM(params['eps'], params['prob'], params['niter'], monotone=params['monotone'])\n","  rdp_func = mech.RenyiDP\n","\n","  # for alpha in range(1, 200):\n","  #   print('rdp_func({})={}'.format(alpha, rdp_func(alpha)))\n","\n","  approxdp = rdp_to_approxdp(rdp_func, alpha_max=200, BBGHS_conversion=True)\n","\n","  return approxdp(delta)"],"metadata":{"id":"ymyLv6gts-Z1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["delta = 5e-5\n","prob = 400/14732\n","\n","privacy_params = {\n","    'eps': 1.65,\n","    'sigma': 1,\n","    'prob': prob,\n","    'niter': 100,\n","    'monotone': False\n","}\n","\n","compose_subsampled_EM_to_approxDP(privacy_params, delta=delta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6HhHcCl1maI","executionInfo":{"status":"ok","timestamp":1710032959951,"user_tz":300,"elapsed":785,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"088b0557-00fe-4c8b-a37e-aeaab1178be1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.9984485729734365"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["delta = 5e-5\n","prob = 400/14732\n","\n","privacy_params = {\n","    'eps': 2.67,\n","    'sigma': 1,\n","    'prob': prob,\n","    'niter': 100,\n","    'monotone': False\n","}\n","\n","compose_subsampled_EM_to_approxDP(privacy_params, delta=delta)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFl2bL2by1Ps","executionInfo":{"status":"ok","timestamp":1710032963069,"user_tz":300,"elapsed":1474,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"9837c51d-cf6a-4497-ca22-b602e31f8ec5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7.9957427730570085"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["### KSA-EM"],"metadata":{"id":"DHHEJlx0mP0q"}},{"cell_type":"code","source":["import json\n","from sklearn.metrics.pairwise import cosine_similarity\n","label_path = path + \"100_test_truth.json\"\n","zero_shot_path = path + \"gpt3da-0shot-test100.json\"\n","four_shot_path = path + \"gpt3da-4shot-100ensemble-100test2.json\"\n","\n","with open(label_path, 'r') as f:\n","  label = json.load(f)\n","with open(zero_shot_path, 'r') as f:\n","  zero_pred_f = json.load(f)\n","zero_pred = []\n","for i in range(len(zero_pred_f)):\n","  zero_pred.append(zero_pred_f[str(i)]['prediction'])\n","with open(four_shot_path, 'r') as f:\n","  four_pred_f = json.load(f)\n","four_pred = []\n","for i in range(len(four_pred_f)):\n","  four_pred.append(four_pred_f[str(i)]['prediction'])\n","print(label[0])\n","print(zero_pred[0])\n","print(four_pred[0])\n","fourshot_pred = four_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vrX7RVunmuu2","executionInfo":{"status":"ok","timestamp":1710032982751,"user_tz":300,"elapsed":1553,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"5104132f-6046-4352-98c5-9fb8c779c4ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Edson is booking his ticket now.\n","\n","Joyce shared a link with Michael and Edson, and Edson was excited to find out that it was a good deal and decided to book a ticket.\n"," Joyce shared a link with Michael and Edson, who found it to be a good deal and decided to book a ticket.\n"]}]},{"cell_type":"code","source":["def make_diff_matrix(item_counts, k):\n","  \"\"\"Makes diff matrix where diff_matrix[i,j] = c_i - c_j + uniquifying term.\n","\n","  Args:\n","    item_counts: Array of item counts, sorted in decreasing order.\n","    k: Number of top counts desired.\n","\n","  Returns:\n","    k x d matrix diff_matrix where diff_matrix[i,j] = c_i - c_j + (d(k-i-1) + j\n","    + 1) / (2dk) and c_1 <= c_2 <= ... <= c_d. diff_matrix is therefore strictly\n","    increasing along rows and strictly decreasing down columns. Note that the\n","    added uniquifying term is determined entirely by d, i, j, k and is therefore\n","    data-independent. Moreover, since the counts are integers, their differences\n","    are integers as well; the uniquifying terms, which have maximum\n","    dk / (2dk) = 0.5, therefore do not change the relative order of any\n","    non-identical count differences.\n","  \"\"\"\n","  d = len(item_counts)\n","  base_along_row = np.arange(1, d + 1)\n","  base_down_col = np.arange(k - 1, -1, -1) * d\n","  uniquifying_terms = (base_along_row[np.newaxis, :] +\n","                       base_down_col[:, np.newaxis]) / (2 * d * k)\n","  return (item_counts[:k, np.newaxis] -\n","          item_counts[np.newaxis, :]) + uniquifying_terms\n","\n","def get_diffs_to_positions(diff_matrix):\n","  \"\"\"Computes array a where diff_matrix[a[0][i], a[1][i]] = sorted_diffs[i].\n","\n","  Args:\n","    diff_matrix: Matrix of distinct count differences.\n","\n","  Returns:\n","    Array a where diff_matrix[a[0][i], a[1][i]] = sorted_diffs[i], where\n","    sorted_diffs contains all entries of diff_matrix in increasing order.\n","  \"\"\"\n","  # The below line of the code runs in time O(d * k * log(dk)). This could be\n","  # implemented more efficiently, leveraging the fact that diff_matrix is\n","  # strictly increasing along rows. This property allows us to use k-way merging\n","  # (https://en.wikipedia.org/wiki/K-way_merge_algorithm), which can bring the\n","  # runtime down to O(d * k * log(k)). However, since this function is not a\n","  # practical bottleneck in the code, we leave it as-is for now.\n","  return np.unravel_index(np.argsort(diff_matrix, axis=None), diff_matrix.shape)\n","\n","def brute_compute_log_diff_counts(diff_matrix, sorted_diffs):\n","  \"\"\"Computes array of log(# sequences w/ diff) for diff in diff_matrix.\n","\n","  Args:\n","    diff_matrix: Matrix of distinct count differences.\n","    sorted_diffs: Diffs from diff_matrix, in increasing order.\n","\n","  Returns:\n","    Array log_counts where, for array sorted_diffs of diffs from diff_matrix\n","    sorted in increasing order,\n","    log_counts[i] = log(# of sequences where largest count difference is diff\n","    from sorted_diffs[i]), computed using brute force.\n","  \"\"\"\n","  k, d = diff_matrix.shape\n","  possible_sequences = itertools.permutations(np.arange(d), k)\n","  diffs_to_counts = np.zeros(d * k)\n","  for sequence in possible_sequences:\n","    diff = np.amax(\n","        [diff_matrix[row_idx, sequence[row_idx]] for row_idx in np.arange(k)])\n","    diff_idx = np.searchsorted(sorted_diffs, diff)\n","    diffs_to_counts[diff_idx] += 1\n","  # Ignore warnings from taking log(0). This produces -np.inf as intended.\n","  with np.errstate(divide=\"ignore\"):\n","    return np.log(diffs_to_counts)\n","\n","def compute_log_diff_counts(diff_matrix, diffs_to_positions):\n","  \"\"\"Computes array of log(sequence count) for each diff in diff_matrix.\n","\n","  Args:\n","    diff_matrix: Matrix of distinct count differences.\n","    diffs_to_positions: Dictionary mapping diffs to positions in diff_matrix.\n","\n","  Returns:\n","    Array log_counts where, for array sorted_diffs of diffs from diff_matrix\n","    sorted in decreasing order, log_counts[i] = log(# of sequences where largest\n","    count difference is diff from sorted_diffs[i]), computed using Lemma 3.7\n","    (the definition of \\tilde{m}) from the paper.\n","\n","  Raises:\n","    RuntimeError: ns vector never filled.\n","  \"\"\"\n","  k, d = diff_matrix.shape\n","  num_diffs = d * k\n","  log_diff_counts = np.empty(num_diffs)\n","  log_ns = np.empty(k)\n","  indices_filled = set()\n","  last_diff_idx_processed = -1\n","  # Ignore warnings from, respectively, taking logs of 0 or negative numbers.\n","  # log(0) becomes -np.inf as intended, and log(<0) becomes nan and is ignored.\n","  with np.errstate(divide=\"ignore\"):\n","    with np.errstate(invalid=\"ignore\"):\n","      updates = np.log((diffs_to_positions[1] + 1) - diffs_to_positions[0])\n","  for (diff_idx, i, u) in zip(range(num_diffs), diffs_to_positions[0], updates):\n","    if np.isnan(u):\n","      continue\n","    log_ns[i] = u\n","    indices_filled.add(i)\n","    if len(indices_filled) == k:\n","      last_diff_idx_processed = diff_idx\n","      break\n","  if last_diff_idx_processed == -1:\n","    raise RuntimeError(\"ns vector never filled\")\n","  log_diff_counts[:last_diff_idx_processed] = -np.inf\n","  log_ns_sum = np.sum(log_ns)\n","  for (diff_idx, i, u) in zip(\n","      range(last_diff_idx_processed, num_diffs),\n","      diffs_to_positions[0][diff_idx:], updates[diff_idx:]):\n","    log_ns_sum -= log_ns[i]\n","    log_diff_counts[diff_idx] = log_ns_sum\n","    log_ns[i] = u\n","    log_ns_sum += log_ns[i]\n","  return log_diff_counts\n","\n","def racing_sample(log_terms):\n","  \"\"\"Numerically stable method for sampling from an exponential distribution.\n","\n","  Args:\n","    log_terms: Array of terms of form log(coefficient) - (exponent term).\n","\n","  Returns:\n","    A sample from the exponential distribution determined by terms. See\n","    Algorithm 1 from the paper \"Duff: A Dataset-Distance-Based\n","    Utility Function Family for the Exponential Mechanism\"\n","    (https://arxiv.org/pdf/2010.04235.pdf) for details; each element of terms is\n","    analogous to a single log(lambda(A_k)) - (eps * k/2) in their algorithm.\n","\n","  Raises:\n","    RuntimeError: encountered inf or nan min time.\n","  \"\"\"\n","  race_times = np.log(np.log(\n","      1.0 / np.random.uniform(size=log_terms.shape))) - log_terms\n","  winner = np.argmin(race_times)\n","  min_time = race_times[winner]\n","  if np.isnan(min_time) or np.isinf(min_time):\n","    raise RuntimeError(\n","        \"Racing sample encountered inf or nan min time: {}\".format(min_time))\n","  return winner\n","\n","def sample_diff_idx(log_diff_counts, sorted_diffs, epsilon, sensitivity):\n","  \"\"\"Racing samples a diff index from the exponential mechanism.\n","\n","  Args:\n","    log_diff_counts: Array of log(# sequences with diff) for each diff in\n","      sorted_diffs.\n","    sorted_diffs: Increasing array of possible diffs.\n","    epsilon: Privacy parameter epsilon.\n","    neighbor_type: Available neighbor types are defined in the NeighborType\n","      enum.\n","\n","  Returns:\n","    Index idx sampled from, for diff = sorted_diffs[idx], distribution\n","    P(diff) ~ count[diff] * exp(-epsilon * floor(diff) / 2).\n","  \"\"\"\n","  return racing_sample(log_diff_counts - (epsilon * np.floor(sorted_diffs) /\n","                                          (2 * sensitivity)))\n","\n","def sample_max_expo_distribution(expo_lambda, log_num_trials):\n","  \"\"\"Computes max values from (simulated) draws from the expo distribution.\n","\n","  Args:\n","    expo_lambda: Exponential distribution parameter; PDF of the distribution is\n","      p(x) = expo_lambda * exp(-expo_lambda * x).\n","    log_num_trials: Array where each entry is the log of the number of\n","      (simulated) draws from the exponential distribution to use in producing a\n","      max value.\n","\n","  Returns:\n","    An array where entry i represents the max over exp(log_num_trials[i]) draws\n","    from the exponenttial distribution with parameter expo_lambda.\n","\n","  Raises:\n","    RuntimeError: result contains inf or nan.\n","  \"\"\"\n","  # Rather than actually sampling from the exponential distribution num_trials\n","  # times and taking the max (an O(num_trials) operation), we can simply draw\n","  # from a distribution that directly represents this max (an O(1) operation).\n","  # The probability that num_trials independent draws from a distribution are\n","  # all <= x is the CDF of that distribution raised to the n-th power.  Hence,\n","  # the CDF for the max that we need is the exponential distribution CDF raised\n","  # to the n-th power.  Inverting this CDF and plugging in a sample from the\n","  # uniform distribution on [0, 1] gives the desired max.\n","  num_results = len(log_num_trials)\n","  # The below line is more numerically stable than 1 / np.exp(log_num_trials).\n","  inverse_num_trials = np.exp(-log_num_trials)\n","  log_uniform_draws = np.log(np.random.uniform(size=num_results))\n","  results = -np.log(-np.expm1(\n","      np.multiply(inverse_num_trials, log_uniform_draws))) / expo_lambda\n","  if np.any(np.isnan(results)) or np.any(np.isinf(results)):\n","    raise RuntimeError(\n","        \"Max expo sampler result contains inf or nan: {}\".format(results))\n","  return results\n","\n","def sample_diff_idx_via_pnf(log_diff_counts, sorted_diffs, epsilon,\n","                            sensitivity):\n","  \"\"\"Samples an index of sorted_diffs according to permute-and-flip (PNF).\n","\n","  Args:\n","    log_diff_counts: Array of log(# sequences with diff) for each diff in\n","      sorted_diffs.\n","    sorted_diffs: Increasing array of possible diffs.\n","    epsilon: Privacy parameter epsilon.\n","    neighbor_type: Available neighbor types are defined in the NeighborType\n","      enum.\n","\n","  Returns:\n","    Index idx into sorted_diffs, sampled according to the permute-and-flip\n","    mechanism.\n","\n","  Raises:\n","    RuntimeError: No noised value exceeded -infinity.\n","  \"\"\"\n","  expo_lambda = epsilon / (2 * sensitivity)\n","\n","  # Exclude entries with a count of zero.\n","  nonzero_count_indicator = ~np.isneginf(log_diff_counts)\n","\n","  # Permute-and-flip is identical to report-noisy-max with exponential noise;\n","  # see the paper \"The Permute-and-Flip Mechanism Is Identical to\n","  # Report-Noisy-Max with Exponential Noise\"\n","  # (https://arxiv.org/pdf/2105.07260.pdf) for details.  The latter formulation\n","  # is simpler to implement efficiently, so that is what we use internally here.\n","  # Specifically, we draw a max noise value for each of the utility values (the\n","  # diffs), and add this max to the utility.  We then return the index where\n","  # this results in the largest noisy utility value.\n","  utilities = -np.floor(sorted_diffs[nonzero_count_indicator])\n","  noisy_utilities = utilities + sample_max_expo_distribution(\n","      expo_lambda, log_diff_counts[nonzero_count_indicator])\n","  if np.all(np.isneginf(noisy_utilities)):\n","    raise RuntimeError(\"No noised value exceeded -infinity\")\n","  nonzero_count_indices = np.flatnonzero(nonzero_count_indicator)\n","  return nonzero_count_indices[np.argmax(noisy_utilities)]\n","\n","def sequence_from_diff(diff,\n","                       diff_row,\n","                       diff_col,\n","                       diff_matrix,\n","                       sampler=lambda ary: np.random.choice(ary, 1)):\n","  \"\"\"Samples a sequence with given diff uniformly at random.\n","\n","  Args:\n","    diff: Diff (negative utility) of sequence to sample.\n","    diff_row: diff_matrix[diff_row, diff_col] = diff.\n","    diff_col: diff_matrix[diff_row, diff_col] = diff.\n","    diff_matrix: Matrix of distinct count differences.\n","    sampler: Function that selects an item from an array. Default value is\n","      uniform random choice.\n","\n","  Returns:\n","    Array of item indices forming a sequence with utility -diff.\n","  \"\"\"\n","  k = len(diff_matrix)\n","  sequence = np.full(k, diff_col)\n","  ts = [\n","      np.searchsorted(diff_matrix[row, :], diff, side=\"right\")\n","      for row in range(k)\n","  ]\n","  for row in range(k):\n","    if row != diff_row:\n","      # The below line of the code runs in time O(dk), which technically makes\n","      # the runtime of the sequence_from_diff function O(dk^2). This could be\n","      # implemented more efficiently, bringing the runtime of the function down\n","      # to O(dk). However, this is not a practical bottleneck in the code, so we\n","      # leave it as-is for now.\n","      to_sample = [i for i in range(ts[row]) if i not in sequence]\n","      sequence[row] = sampler(to_sample)\n","  return sequence\n","\n","def joint(item_counts,\n","          k,\n","          epsilon,\n","          neighbor_type,\n","          sample_diff_idx_func=sample_diff_idx):\n","  \"\"\"Applies joint exponential mechanism to return sequence of top-k items.\n","\n","  Args:\n","    item_counts: Array of item counts.\n","    k: Number of items with top counts to return.\n","    epsilon: Privacy parameter epsilon.\n","    neighbor_type: Available neighbor types are defined in the NeighborType\n","      enum.\n","    sample_diff_idx_func: Function to call for sampling a utility value.\n","\n","  Returns:\n","    Array of k item indices as estimated by the joint exponential mechanism.\n","  \"\"\"\n","  # Sort the counts in non-increasing order.\n","  sort_indices = np.argsort(item_counts)[::-1]\n","  item_counts = item_counts[sort_indices]\n","\n","  # Note that the diff_matrix here is the negative of the \\tilde{U} matrix from\n","  # the paper.\n","  diff_matrix = make_diff_matrix(item_counts, k)\n","  diffs_to_positions = get_diffs_to_positions(diff_matrix)\n","  log_diff_counts = compute_log_diff_counts(diff_matrix, diffs_to_positions)\n","  sorted_diffs = diff_matrix[diffs_to_positions]\n","  diff_idx = sample_diff_idx_func(log_diff_counts, sorted_diffs, epsilon,\n","                                  neighbor_type)\n","  diff_row, diff_col = diffs_to_positions[0][diff_idx], diffs_to_positions[1][\n","      diff_idx]\n","  sequence = sequence_from_diff(sorted_diffs[diff_idx], diff_row, diff_col,\n","                                diff_matrix)\n","  # Convert the indices returned by sequence_from_diff to the original item ids.\n","  # return sort_indices[sequence]\n","  return sequence\n","\n","def pnf_joint(item_counts, k, epsilon, neighbor_type):\n","  \"\"\"Applies joint permute-and-flip mechanism to return sequence of top-k items.\n","\n","     Internals are identical to the exponential mechanism version of joint,\n","     except for the sample_diff_idx method.\n","\n","  Args:\n","    item_counts: Array of item counts.\n","    k: Number of items with top counts to return.\n","    epsilon: Privacy parameter epsilon.\n","    neighbor_type: Available neighbor types are defined in the NeighborType\n","      enum.\n","\n","  Returns:\n","    Array of k item indices as estimated by the joint permute-and-flip\n","    mechanism.\n","  \"\"\"\n","  return joint(item_counts, k, epsilon, neighbor_type, sample_diff_idx_via_pnf)"],"metadata":{"id":"kK1QiuGzmOwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds_size = 100 #test data size\n","repeat = 1\n","ensemble = 100\n","from tqdm import tqdm\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","import string\n","import random\n","import openai\n","import time\n","import evaluate\n","metric = evaluate.load(\"rouge\")\n","\n","stopword_set = set(stopwords.words('english'))\n","# eps_1 = 0.77, eps_3 = 1.65, eps_8 = 2.67\n","rouge1, rouge2, rougeL, rougeLsum = [],[],[],[]\n","all_eps = [1.65]\n","for index, num_ensemble in enumerate([100]):\n","     eps =  all_eps[index]\n","     for _ in range(3):\n","          # get predictions from json file\n","          final_pred = []\n","          count_threshold_list = []\n","          for i in tqdm(range(ds_size)):\n","                # all_tokens = {} # key: token, value: count\n","                sentence = zero_pred_f[str(i)][\"origin_prompt\"]\n","                all_tokens = {}\n","                tokens = nltk.word_tokenize(sentence)\n","                onegrams = set(ngrams(tokens, 1))\n","                # onegrams = set(onegrams)\n","                # making onegrams a set to avoid duplicate tokens\n","                for token in onegrams:\n","                    # only add one gram for one sentence\n","                    if token in all_tokens:\n","                          all_tokens[token] += 0\n","                    else:\n","                          all_tokens[token] = 0\n","                for j in range(num_ensemble):\n","                      sentence = fourshot_pred[i*ensemble+j]\n","                      tokens = nltk.word_tokenize(sentence)\n","                      onegrams = set(ngrams(tokens, 1))\n","                      # onegrams = set(onegrams)\n","                      # making onegrams a set to avoid duplicate tokens\n","                      for token in onegrams:\n","                          # only add one gram for one sentence\n","                          if token in all_tokens:\n","                                all_tokens[token] += 1\n","                #print(all_tokens)\n","                all_tokens_sorted = sorted(all_tokens.items(), key=lambda x: x[1], reverse=True)\n","                #print(all_tokens_sorted)\n","                # ignore those non-words tokens\n","                filtered_tokens = {}\n","                for token, count in all_tokens_sorted:\n","                      if not all(word in string.punctuation for word in token) and token[0] not in stopword_set:\n","                          filtered_tokens[token] = count\n","                filtered_tokens_sorted = sorted(filtered_tokens.items(), key=lambda x: x[1], reverse=True)\n","                item_counts = np.array([count for token, count in filtered_tokens_sorted])\n","                joint_out = joint(item_counts, k=min(10, len(item_counts)), epsilon=eps, neighbor_type=1)\n","                filtered_tokens = np.array(filtered_tokens_sorted, dtype=object)[joint_out]\n","                filtered_tokens = [token_tuple[0][0] for token_tuple in filtered_tokens]\n","                prompt = '['\n","                for token in filtered_tokens:\n","                      prompt += token + ', '\n","                prompt = prompt[:-2] + ']'\n","                prompt = prompt + '\\nThe summary is:'\n","                zero_shot_sentence = zero_pred_f[str(i)]['origin_prompt'].replace('Summarize the above dialogue:','Summarize the above dialogue with the following word suggestions ranked by their frequency from high to low:')\n","                #print(zero_shot_sentence)\n","                final_prompt = zero_shot_sentence  + prompt\n","                # if i < 5 :\n","                #   print(final_prompt, label[i])\n","\n","                ################################################################################\n","                ##### Please check your code and run this api to complete the experiment'#######\n","                ################################################################################\n","\n","                openai.api_key = \"\"\n","\n","                max_attempts = 5\n","                current_attempt = 1\n","                # pred = openai.Completion.create(\n","                #      engine=\"text-davinci-003\",\n","                #      prompt=final_prompt,\n","                #      temperature=0,\n","                #      max_tokens=256,\n","                #      top_p=1.0,\n","                #      frequency_penalty=0.0,\n","                #      presence_penalty=0.0\n","                # )\n","                # final_pred.append(pred['choices'][0]['text'])\n","\n","                while current_attempt <= max_attempts:\n","                    try:\n","                        pred = openai.Completion.create(\n","                            engine=\"text-davinci-003\",\n","                            prompt=final_prompt,\n","                            temperature=0,\n","                            max_tokens=256,\n","                            top_p=1.0,\n","                            frequency_penalty=0.0,\n","                            presence_penalty=0.0\n","                        )\n","                        result = pred['choices'][0]['text']\n","                        final_pred.append(result)\n","                        break  # Exit the loop if the query is successful\n","                    except Exception as e:\n","                        print(f\"Query failed with error: {e}\")\n","                        current_attempt += 1  # Increment the attempt counter\n","                        if current_attempt <= max_attempts:\n","                            print(\"Retrying in 3 seconds...\")\n","                            time.sleep(3)  # Wait for 3 seconds before the next query\n","\n","            #final_pred.append(\"test\") # comment this to run.\n","\n","            # print(final_pred[-1])\n","\n","          score = metric.compute(predictions=final_pred, references=label)\n","          rouge1.append(score['rouge1'])\n","          rouge2.append(score['rouge2'])\n","          rougeL.append(score['rougeL'])\n","          rougeLsum.append(score['rougeLsum'])\n","     print(\"rouge1's mean: \",    np.mean(np.array(rouge1)), \"rouge1's std: \", np.std(np.array(rouge1)))\n","     print(\"rouge2's mean: \",    np.mean(np.array(rouge2)), \"rouge2's std: \", np.std(np.array(rouge2)))\n","     print(\"rougeL's mean: \",    np.mean(np.array(rougeL)), \"rougeL's std: \", np.std(np.array(rougeL)))\n","     print(\"rougeLsum's mean: \", np.array(rougeLsum), \"rougeLsum's std: \", np.std(np.array(rougeLsum)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mvT9LXYumYH0","executionInfo":{"status":"error","timestamp":1710033075232,"user_tz":300,"elapsed":81577,"user":{"displayName":"Tong Wu","userId":"07302789661051936810"}},"outputId":"961b5935-da9a-47bc-e85a-fcd4ec40c5b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","  0%|          | 0/100 [00:00<?, ?it/s]<ipython-input-38-7d27f7f7c2e5>:268: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  sequence[row] = sampler(to_sample)\n"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 1/100 [00:12<20:38, 12.51s/it]"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 2/100 [00:24<20:19, 12.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 3/100 [00:37<20:08, 12.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 4/100 [00:49<19:52, 12.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 5/100 [01:02<19:42, 12.45s/it]"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 6/100 [01:14<19:28, 12.43s/it]"]},{"output_type":"stream","name":"stdout","text":["Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n","Query failed with error: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\n","Retrying in 3 seconds...\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 6/100 [01:20<21:05, 13.47s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-aa7e45907ed7>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mcurrent_attempt\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_attempts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                         pred = openai.Completion.create(\n\u001b[0m\u001b[1;32m     97\u001b[0m                             \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     ) -> requests.Response:\n\u001b[0;32m--> 581\u001b[0;31m         abs_url, headers, data = self._prepare_request_raw(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupplied_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_prepare_request_raw\u001b[0;34m(self, url, supplied_headers, method, params, files, request_id)\u001b[0m\n\u001b[1;32m    560\u001b[0m             )\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Request to OpenAI API\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabs_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_headers\u001b[0;34m(self, method, extra, request_id)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0muser_agent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_app_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         uname_without_node = \" \".join(\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"node\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         uname_without_node = \" \".join(\n\u001b[0;32m--> 474\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"node\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m         )\n\u001b[1;32m    476\u001b[0m         ua = {\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"8mSECxaI6Lxc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bd0ed318ed854ff69dc31b2d235b8366":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_17c2c507c3ae4c529ef998dd3fe6bdf3","IPY_MODEL_a5d4e826021b4bd68586529a8ef94d75","IPY_MODEL_e404dffdbb664ee8bbef52c380419145"],"layout":"IPY_MODEL_6cbd610b9f694f14b1db0da31707fb26"}},"17c2c507c3ae4c529ef998dd3fe6bdf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_576770ae31e3431ca75d8f5cd9af56bb","placeholder":"​","style":"IPY_MODEL_27c46095eadc48b8b085cda1df5dd933","value":"Downloading builder script: 100%"}},"a5d4e826021b4bd68586529a8ef94d75":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_daa05e2310af435b9c209751327e8864","max":6270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_670861f02e3f4e6a9ce4953c6bca1123","value":6270}},"e404dffdbb664ee8bbef52c380419145":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8af23c567e51424eaa16f03003e23bb8","placeholder":"​","style":"IPY_MODEL_d956fe4869d349ebaa4907ff6af41a8d","value":" 6.27k/6.27k [00:00&lt;00:00, 336kB/s]"}},"6cbd610b9f694f14b1db0da31707fb26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"576770ae31e3431ca75d8f5cd9af56bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27c46095eadc48b8b085cda1df5dd933":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"daa05e2310af435b9c209751327e8864":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"670861f02e3f4e6a9ce4953c6bca1123":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8af23c567e51424eaa16f03003e23bb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d956fe4869d349ebaa4907ff6af41a8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}